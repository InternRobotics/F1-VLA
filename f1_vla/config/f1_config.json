{
  "architectures": [
    "F1ForConditionalGeneration"
  ],
  "auto_map": {
    "AutoConfig": "configuration_f1.F1Config",
    "AutoModel": "modeling_f1.F1ForConditionalGeneration"
  },
  "und_expert_config": {
    "transformers_version": "4.48.1",
    "_vocab_size": 257152,
    "bos_token_id": 2,
    "eos_token_id": 1,
    "hidden_size": 2048,
    "image_token_index": 257152,
    "model_type": "paligemma",
    "pad_token_id": 0,
    "projection_dim": 2048,
    "text_config": {
        "hidden_activation": "gelu_pytorch_tanh",
        "hidden_size": 2048,
        "intermediate_size": 16384,
        "model_type": "gemma",
        "num_attention_heads": 8,
        "num_hidden_layers": 18,
        "num_image_tokens": 256,
        "num_key_value_heads": 1,
        "torch_dtype": "float32",
        "vocab_size": 257152
    },
    "vision_config": {
        "hidden_size": 1152,
        "intermediate_size": 4304,
        "model_type": "siglip_vision_model",
        "num_attention_heads": 16,
        "num_hidden_layers": 27,
        "num_image_tokens": 256,
        "patch_size": 14,
        "projection_dim": 2048,
        "projector_hidden_act": "gelu_fast",
        "torch_dtype": "float32",
        "vision_use_head": false
    }
  },
  "gen_expert_config": {
      "attention_bias": false,
      "attention_dropout": 0.0,
      "bos_token_id": 2,
      "eos_token_id": 1,
      "head_dim": 256,
      "hidden_act": "gelu_pytorch_tanh",
      "hidden_activation": "gelu_pytorch_tanh",
      "hidden_size": 1024,
      "initializer_range": 0.02,
      "intermediate_size": 4096,
      "max_position_embeddings": 8192,
      "model_type": "gemma",
      "num_attention_heads": 8,
      "num_hidden_layers": 18,
      "num_key_value_heads": 1,
      "pad_token_id": 0,
      "rms_norm_eps": 1e-06,
      "rope_theta": 10000.0,
      "torch_dtype": "float32",
      "transformers_version": "4.48.1",
      "use_cache": true,
      "vocab_size": 257152,
      "pn": "1_2_3_4_5_6_8_10_13_16",
      "temporal_conv_kernel_size": 4,
      "temporal_conv_stride": 4,
      "num_resolutions": 4,
      "vae": {
        "vocab_size": 4096,
        "z_channels": 32,
        "ch": 160,
        "test_mode": true,
        "share_quant_resi": 4,
        "vae_ckpt": ""
      }
  },
  "act_expert_config": {
      "attention_bias": false,
      "attention_dropout": 0.0,
      "bos_token_id": 2,
      "eos_token_id": 1,
      "head_dim": 256,
      "hidden_act": "gelu_pytorch_tanh",
      "hidden_activation": "gelu_pytorch_tanh",
      "hidden_size": 1024,
      "initializer_range": 0.02,
      "intermediate_size": 4096,
      "max_position_embeddings": 8192,
      "model_type": "gemma",
      "num_attention_heads": 8,
      "num_hidden_layers": 18,
      "num_key_value_heads": 1,
      "pad_token_id": 0,
      "rms_norm_eps": 1e-06,
      "rope_theta": 10000.0,
      "torch_dtype": "float32",
      "transformers_version": "4.48.1",
      "use_cache": true,
      "vocab_size": 257152
  },
  "use_world_model": true,
  "chunk_size": 50,
  "max_action_dim": 32,
  "max_state_dim": 32,
  "tokenizer_max_length": 48,
  "use_cache": true,
  "attention_implementation": "eager",
  "resize_imgs_with_padding": [
    224,
    224
  ],
  "language_tokenizer_path": "",
  "proj_width": 1024,
  "num_steps": 10
}